{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dfbd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e88cd2",
   "metadata": {},
   "source": [
    "# LIST FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a014432",
   "metadata": {},
   "outputs": [],
   "source": [
    "def folder(f_name): #this function creates a folder named \"attacks\" in the program directory.\n",
    "    try:\n",
    "        if not os.path.exists(f_name):\n",
    "            os.makedirs(f_name)\n",
    "    except OSError:\n",
    "        print (\"The folder could not be created!\")\n",
    "\n",
    "def find_the_way(path,file_format,con=\"\"):\n",
    "    files_add = []\n",
    "    # r=root, d=directories, f = files\n",
    "    for r, d, f in os.walk(path):\n",
    "        for file in f:\n",
    "            if file_format in file:\n",
    "                if con in file:\n",
    "                    files_add.append(os.path.join(r, file))  \n",
    "            \n",
    "    return files_add\n",
    "path=\"./pcaps/\"\n",
    "files_add=find_the_way(path,'.pcap')\n",
    "files_add"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e817abb5",
   "metadata": {},
   "source": [
    "# PCAP2JSON "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e503e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder(\"json\")\n",
    "def pcap_parser(oldname):\n",
    "    piece_size=50000\n",
    "    new_name=oldname.replace(\".pcap\",\"_smaller.pcap\")\n",
    "    new_name=new_name.replace(\"\\\\\",\"/\")\n",
    "    command='C:/\\\"Program Files\\\"/Wireshark/editcap.exe -c '+str(piece_size)+\" \\\"\"+oldname+\"\\\" \"+new_name   \n",
    "    os.system(command)\n",
    "    parsed=find_the_way(\"./\",new_name.split(\"/\")[-1][:-5])\n",
    "    return parsed,new_name.split(\"/\")[-1][:-5]\n",
    "def tojson(temp):\n",
    "    temp=temp.replace(\"\\\\\",\"/\")\n",
    "    temp=temp.replace(\"/\",\"-\")\n",
    "    #print(temp)\n",
    "    temp=f\"./json/{temp[2:-4]}json\"\n",
    "    os.system(f'tshark -r {i} -T json > {temp}')\n",
    "    #print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e299a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(files_add):\n",
    "    boyut=os.path.getsize(i)\n",
    "    label_count=0\n",
    "    if boyut >100000000:\n",
    "        file_list,keyword=pcap_parser(i)\n",
    "        for ii in tqdm(file_list):\n",
    "            tojson(ii)\n",
    "        #os.remove(i)\n",
    "    else:tojson(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3347f102",
   "metadata": {},
   "source": [
    "# JSON READER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4df0aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# JSON dosyasını okuma fonksiyonu\n",
    "def read_json_file(file_path):\n",
    "    with open(file_path, 'rb') as json_file:\n",
    "        data = json.load(json_file)\n",
    "    return data\n",
    "\n",
    "# JSON verisini oku\n",
    "json_data = read_json_file(temp)\n",
    "# JSON verisini ekrana yazdır\n",
    "print(json.dumps(json_data, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f9aac0",
   "metadata": {},
   "source": [
    "# JSON2CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1a1bbc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path=\"./json\"\n",
    "files_add=find_the_way(path,'.json')\n",
    "folder(\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e36514e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in  tqdm(files_add):\n",
    "    json_data = read_json_file(i)\n",
    "\n",
    "    # create dataframe\n",
    "    df = pd.json_normalize(json_data)\n",
    "    \n",
    "    temp=i.replace(\"json\",\"csv\")\n",
    "\n",
    "    df.to_csv(temp, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b236312f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"./csv\"\n",
    "files_add=find_the_way(path,'.csv')\n",
    "files_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7408bc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "essiz=[]\n",
    "for i in  tqdm(files_add):\n",
    "    df=pd.read_csv(i)\n",
    "    for ii in df.columns:\n",
    "        if ii not in essiz:\n",
    "            essiz.append(ii)\n",
    "            \n",
    "len(essiz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5949ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(essiz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f81ab6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"./csv\"\n",
    "files_add=find_the_way(path,'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68c6dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "allsize=[]\n",
    "for i in tqdm(files_add):\n",
    "    \n",
    "    mergedFeatures={\"http\":[],\"dns\":[],\"mdns\":[],\"tls\":[],\"urlencoded\":[]}\n",
    "    prefix={\"http\":\"_source.layers.http.\",\"urlencoded\":\"_source.layers.urlencoded-form.Form\",\"dns\":\"_source.layers.dns\",\"mdns\":\"_source.layers.mdns\",\"tls\":\"_source.layers.tls\"}\n",
    "    size=[]\n",
    "    df=pd.read_csv(i)\n",
    "    df=df.dropna(axis=1, how='all')\n",
    "    size.append(df.shape[1])\n",
    "    for ii in prefix:\n",
    "        for iii in df.columns:\n",
    "            if prefix[ii] in  iii:\n",
    "                mergedFeatures[ii].append(iii)\n",
    "        df[prefix[ii]] = df[mergedFeatures[ii]].apply(\n",
    "            lambda x: ','.join(x.dropna().astype(str)),\n",
    "            axis=1\n",
    "        )\n",
    "        size.append(df.shape[1])\n",
    "        for iii in mergedFeatures[ii]:\n",
    "            del df[iii]\n",
    "\n",
    "        #if mergedFeatures[ii]!=[]:            print(i,mergedFeatures[ii])\n",
    "\n",
    "    size.append(df.shape[1])\n",
    "    df.to_csv(i,index=False)\n",
    "    allsize.append(size)\n",
    "for s in allsize:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a3bc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "essiz=[]\n",
    "for i in  tqdm(files_add):\n",
    "    df=pd.read_csv(i)\n",
    "    for ii in df.columns:\n",
    "        if ii not in essiz:\n",
    "            essiz.append(ii)\n",
    "            \n",
    "len(essiz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176f9301",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in essiz:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5302a2f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335079d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee76c6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79c9bf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85997024",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "072ae372",
   "metadata": {},
   "source": [
    "# CSV MERGER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af167747",
   "metadata": {},
   "outputs": [],
   "source": [
    "flag=0\n",
    "for i in  tqdm(files_add):\n",
    "    if flag==0:\n",
    "        df=pd.read_csv(i)\n",
    "        df.to_csv(\"nihai.csv\", index=False)\n",
    "    else:\n",
    "        df=pd.read_csv(\"nihai.csv\")\n",
    "        ek=pd.read_csv(i)\n",
    "        result = pd.concat([df, ek], ignore_index=True, sort=False)\n",
    "        result=result.dropna(axis=1, how='all')\n",
    "        result.to_csv(\"nihai.csv\", index=False)\n",
    "        #result.to_csv(f\"nihai{flag}.csv\", index=False)\n",
    "    flag+=1\n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
