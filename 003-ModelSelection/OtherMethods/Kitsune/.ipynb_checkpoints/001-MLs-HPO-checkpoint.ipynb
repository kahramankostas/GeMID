{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from scipy.stats import randint as sp_randint\n",
    "from scipy.stats import uniform\n",
    "from scipy.stats import uniform as sp_randFloat\n",
    "from sklearn import svm\n",
    "from tqdm import tqdm\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import CategoricalNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from time import time\n",
    "from tabulate import tabulate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_add=['csvs\\\\AD-S1.csv', 'csvs\\\\AD-S2.csv', 'csvs\\\\DI-S1.csv', 'csvs\\\\DI-S2.csv']\n",
    "for i in files_add:\n",
    "    df=pd.read_csv(i)\n",
    "    df = df.groupby('Label').apply(lambda x: x.sample(n=min(1000, len(x))))\n",
    "    df = df.droplevel('Label')\n",
    "    name=i.replace(\"csvs\",\"small\")\n",
    "    df.to_csv(name,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import randint as sp_randInt\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, PredefinedSplit\n",
    "from sklearn.metrics import make_scorer\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "#with open('GA_output_ET.json', 'r') as fp:\n",
    "    #feature_list = json.load(fp)\n",
    "\n",
    "feature_list = ['HH_0.01_covariance_0_1',\n",
    "  'HH_0.01_mean_0',\n",
    "  'HH_0.01_radius_0_1',\n",
    "  'HH_0.01_std_0',\n",
    "  'HH_0.1_covariance_0_1',\n",
    "  'HH_0.1_magnitude_0_1',\n",
    "  'HH_0.1_std_0',\n",
    "  'HH_0.1_weight_0',\n",
    "  'HH_1_magnitude_0_1',\n",
    "  'HH_1_pcc_0_1',\n",
    "  'HH_1_radius_0_1',\n",
    "  'HH_1_std_0',\n",
    "  'HH_1_weight_0',\n",
    "  'HH_3_covariance_0_1',\n",
    "  'HH_3_magnitude_0_1',\n",
    "  'HH_3_radius_0_1',\n",
    "  'HH_3_std_0',\n",
    "  'HH_3_weight_0',\n",
    "  'HH_5_mean_0',\n",
    "  'HH_5_pcc_0_1',\n",
    "  'HH_5_std_0',\n",
    "  'HH_5_weight_0',\n",
    "  'HH_jit_0.01_std',\n",
    "  'HH_jit_0.1_mean',\n",
    "  'HH_jit_0.1_std',\n",
    "  'HH_jit_1_mean',\n",
    "  'HH_jit_1_std',\n",
    "  'HH_jit_1_weight',\n",
    "  'HH_jit_3_std',\n",
    "  'HH_jit_5_std',\n",
    "  'HpHp_0.01_magnitude_0_1',\n",
    "  'HpHp_0.01_radius_0_1',\n",
    "  'HpHp_0.01_weight_0',\n",
    "  'HpHp_0.1_covariance_0_1',\n",
    "  'HpHp_0.1_magnitude_0_1',\n",
    "  'HpHp_0.1_mean_0',\n",
    "  'HpHp_0.1_pcc_0_1',\n",
    "  'HpHp_0.1_std_0',\n",
    "  'HpHp_0.1_weight_0',\n",
    "  'HpHp_1_covariance_0_1',\n",
    "  'HpHp_1_magnitude_0_1',\n",
    "  'HpHp_1_mean_0',\n",
    "  'HpHp_1_pcc_0_1',\n",
    "  'HpHp_1_radius_0_1',\n",
    "  'HpHp_1_std_0',\n",
    "  'HpHp_3_pcc_0_1',\n",
    "  'HpHp_3_radius_0_1',\n",
    "  'HpHp_5_pcc_0_1',\n",
    "  'HpHp_5_radius_0_1',\n",
    "  'HpHp_5_std_0',\n",
    "  'HpHp_5_weight_0',\n",
    "  'MI_dir_0.01_mean',\n",
    "  'MI_dir_0.01_std',\n",
    "  'MI_dir_0.01_weight',\n",
    "  'MI_dir_0.1_mean',\n",
    "  'MI_dir_0.1_std',\n",
    "  'MI_dir_0.1_weight',\n",
    "  'MI_dir_1_std',\n",
    "  'MI_dir_1_weight',\n",
    "  'MI_dir_3_mean',\n",
    "  'MI_dir_5_mean',\n",
    "  'MI_dir_5_std',\n",
    "  'MI_dir_5_weight',\n",
    "  'Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list={\"HPO\":['./small/AD-S1.csv','./small/AD-S2.csv']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_random_search(model, params, x_train, y_train):\n",
    "    #grid = GridSearchCV(model, params, cv = ps, n_jobs = -1, scoring = score, verbose = 0, refit = False)\n",
    "    grid =RandomizedSearchCV(model, param_grid, cv=ps,scoring = 'f1_macro')\n",
    "    grid.fit(x_train, y_train)\n",
    "    return (grid.best_params_, round(grid.best_score_,8),grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_the_way(path,file_format,con=\"\"):\n",
    "    files_add = []\n",
    "    # r=root, d=directories, f = files\n",
    "    for r, d, f in os.walk(path):\n",
    "        for file in f:\n",
    "            if file_format in file:\n",
    "                if con in file:\n",
    "                    files_add.append(os.path.join(r, file))  \n",
    "            \n",
    "    return files_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomizedSearchCV  DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HPO\n",
      "HYPERPARAMETERS                     F1 Score             Time     No      \n",
      "default                             0.8721576940691237   43.347   24      \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 50/50 [1:48:57<00:00, 130.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    criterion      max_depth    max_features    min_samples_split        F1        Std     Time    No  Attack\n",
      "--  -----------  -----------  --------------  -------------------  --------  ---------  -------  ----  --------\n",
      " 0  entropy               16              33                    5  0.892835  0.0213679  194.999    19  HPO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lines=[['criterion', 'max_depth', 'max_features', 'min_samples_split', \"F1\",\"Std\",\"Time\",\"No\",\"Attack\"]]\n",
    "\n",
    "for j in file_list:\n",
    "    print(j)\n",
    "    df=pd.read_csv(file_list[j][0],usecols=feature_list) \n",
    "    X_train = df.iloc[:,0:-1]\n",
    "    df['Label'] = df['Label'].astype('category')\n",
    "    y_train=df['Label'].cat.codes  \n",
    "\n",
    "\n",
    "    df=pd.read_csv(file_list[j][1],usecols=feature_list) \n",
    "    X_test = df.iloc[:,0:-1]\n",
    "    df['Label'] = df['Label'].astype('category')\n",
    "    y_test=df['Label'].cat.codes  \n",
    "    \n",
    "    \n",
    "    \n",
    "    X= np.concatenate([X_train, X_test])\n",
    "    test_fold = [-1 for _ in range(X_train.shape[0])] + [0 for _ in range(X_test.shape[0])]\n",
    "    y = np.concatenate([y_train, y_test])\n",
    "    ps = PredefinedSplit(test_fold)\n",
    "    \n",
    "    \n",
    "    print ('%-35s %-20s %-8s %-8s' % (\"HYPERPARAMETERS\",\"F1 Score\", \"Time\", \"No\"))\n",
    "\n",
    "    nfolds=10\n",
    "    param_grid = { 'criterion':['gini','entropy'],\n",
    "                      \"max_depth\":np.linspace(1, 32, 32, endpoint=True).astype(int),\n",
    "                     \"min_samples_split\": sp_randint(2,10),#uniform(0.1,1 ),\n",
    "                        # \"min_samples_leafs\" : np.linspace(0.1, 0.5, 5, endpoint=True)\n",
    "                        \"max_features\" : sp_randint(1,X_train.shape[1])}\n",
    "\n",
    "    second=time()\n",
    "    f1=[]\n",
    "    clf=DecisionTreeClassifier()\n",
    "    for ii in range(25):\n",
    "        clf.fit(X_train, y_train)\n",
    "        predict =clf.predict(X_test)\n",
    "        f1.append(sklearn.metrics.f1_score(y_test, predict,average= \"macro\") )\n",
    "    f1=sum(f1)/len(f1)   \n",
    "    #if f1>0.76:\n",
    "    print('%-35s %-20s %-8s %-8s' % (\"default\",f1,round(time()-second,3),ii))\n",
    "    ######################################################################################################################\n",
    "    for i in tqdm(range(50)):\n",
    "        second=time()\n",
    "        a,b,clf=run_random_search(DecisionTreeClassifier(),param_grid,X,y)\n",
    "        f1=[]\n",
    "        for ii in range(10):\n",
    "            clf.fit(X_train, y_train)\n",
    "            predict =clf.predict(X_test)\n",
    "            f1.append(sklearn.metrics.f1_score(y_test, predict,average= \"macro\") )\n",
    "        f1_result=sum(f1)/len(f1)   \n",
    "        f1=np.array(f1)\n",
    "        stndtd=f1.std()\n",
    "        temp=list(a.values())\n",
    "        #print('%-90s %-20s %-8s %-8s' % (a,f1_result,round(time()-second,3),i))\n",
    "        temp=temp+[f1_result,stndtd,round(time()-second,3),i,j]\n",
    "        lines.append(temp)\n",
    "\n",
    "        #if f1>0.76:\n",
    "results = pd.DataFrame (lines[1:], columns = lines[0])\n",
    "results.to_csv(\"DT_HPO.csv\",index=False)\n",
    "\n",
    "final_parametres=[['criterion', 'max_depth', 'max_features', 'min_samples_split', \"F1\",\"Std\",\"Time\",\"No\",\"Attack\"]]\n",
    "for i in results[\"Attack\"].unique():\n",
    "    df=results[results[\"Attack\"]==i]\n",
    "    m=df[\"F1\"].max()\n",
    "    df=df[df[\"F1\"]==m]\n",
    "    m=df[\"max_depth\"].min()\n",
    "    df=df[df[\"max_depth\"]==m]  \n",
    "    final_parametres.append(list(df.values)[0])\n",
    "results = pd.DataFrame (final_parametres[1:], columns=  final_parametres[0])\n",
    "print (tabulate(results, headers=list(results.columns)))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomizedSearchCV RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HPO\n",
      "HYPERPARAMETERS                     F1 Score             Time     No      \n",
      "default                             0.9109616350939413   10.769   0       \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 10/10 [1:03:58<00:00, 383.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    bootst    criter      max_depth    max_features    min_samp_split    n_estimators        F1         Std     Time    No  Attack\n",
      "--  --------  --------  -----------  --------------  ----------------  --------------  --------  ----------  -------  ----  --------\n",
      " 0  False     entropy            20               2                 2              79  0.923715  0.00643388  228.121     0  HPO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lines=[['bootst', 'criter', 'max_depth', 'max_features',\"min_samp_split\",\"n_estimators\", \"F1\",\"Std\",\"Time\",\"No\",\"Attack\"]]\n",
    "\n",
    "\n",
    "\n",
    "for j in file_list:\n",
    "    print(j)\n",
    "    \n",
    "    \n",
    "    df=pd.read_csv(file_list[j][0],usecols=feature_list) \n",
    "    X_train = df.iloc[:,0:-1]\n",
    "    df['Label'] = df['Label'].astype('category')\n",
    "    y_train=df['Label'].cat.codes  \n",
    "\n",
    "\n",
    "    df=pd.read_csv(file_list[j][1],usecols=feature_list) \n",
    "    X_test = df.iloc[:,0:-1]\n",
    "    df['Label'] = df['Label'].astype('category')\n",
    "    y_test=df['Label'].cat.codes  \n",
    "    \n",
    "    \n",
    "    \n",
    "    X= np.concatenate([X_train, X_test])\n",
    "    test_fold = [-1 for _ in range(X_train.shape[0])] + [0 for _ in range(X_test.shape[0])]\n",
    "    y = np.concatenate([y_train, y_test])\n",
    "    ps = PredefinedSplit(test_fold)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    print ('%-35s %-20s %-8s %-8s' % (\"HYPERPARAMETERS\",\"F1 Score\", \"Time\", \"No\"))\n",
    "\n",
    "\n",
    "\n",
    "    # use a full grid over all parameters\n",
    "    param_grid = {\"max_depth\":np.linspace(1, 32, 32, endpoint=True).astype(int),\n",
    "                  \"n_estimators\" : sp_randint(1, 200),\n",
    "                  \"max_features\": sp_randint(1, 11),\n",
    "                  \"min_samples_split\":sp_randint(2, 11),\n",
    "                  \"bootstrap\": [True, False],\n",
    "                  \"criterion\": [\"gini\", \"entropy\"]}\n",
    "    second=time()\n",
    "    f1=[]\n",
    "    clf=RandomForestClassifier()\n",
    "    for ii in range(1):\n",
    "        clf.fit(X_train, y_train)\n",
    "        predict =clf.predict(X_test)\n",
    "        f1.append(sklearn.metrics.f1_score(y_test, predict,average= \"macro\") )\n",
    "    f1=sum(f1)/len(f1)   \n",
    "    #if f1>0.76:\n",
    "    print('%-35s %-20s %-8s %-8s' % (\"default\",f1,round(time()-second,3),ii))\n",
    "\n",
    "\n",
    "    \n",
    "    ######################################################################################################################\n",
    "    for i in tqdm(range(10)):\n",
    "        second=time()\n",
    "        a,b,clf=run_random_search(RandomForestClassifier(),param_grid,X,y)\n",
    "        f1=[]\n",
    "        for ii in range(5):\n",
    "            clf.fit(X_train, y_train)\n",
    "            predict =clf.predict(X_test)\n",
    "            f1.append(sklearn.metrics.f1_score(y_test, predict,average= \"macro\") )\n",
    "        f1_result=sum(f1)/len(f1)   \n",
    "        f1=np.array(f1)\n",
    "        stndtd=f1.std()\n",
    "        temp=list(a.values())\n",
    "        #print('%-90s %-20s %-8s %-8s' % (a,f1_result,round(time()-second,3),i))\n",
    "        temp=temp+[f1_result,stndtd,round(time()-second,3),i,j]\n",
    "        lines.append(temp)\n",
    "\n",
    "        #if f1>0.76:\n",
    "results = pd.DataFrame (lines[1:], columns = lines[0])\n",
    "results.to_csv(\"RF_HPO.csv\",index=False)\n",
    "\n",
    "final_parametres=[['bootst', 'criter', 'max_depth', 'max_features',\"min_samp_split\",\"n_estimators\", \"F1\",\"Std\",\"Time\",\"No\",\"Attack\"]]\n",
    "\n",
    "for i in results[\"Attack\"].unique():\n",
    "    df=results[results[\"Attack\"]==i]\n",
    "    m=df[\"F1\"].max()\n",
    "    df=df[df[\"F1\"]==m]\n",
    "    m=df[\"max_depth\"].min()\n",
    "    df=df[df[\"max_depth\"]==m]  \n",
    "    final_parametres.append(list(df.values)[0])\n",
    "results = pd.DataFrame (final_parametres[1:], columns=  final_parametres[0])\n",
    "print (tabulate(results, headers=list(results.columns)))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HPO\n",
      "default                             0.9230876669528796   13.459   0       \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████████████                                                                | 1/5 [24:25<1:37:42, 1465.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 1100, 'min_child_weight': 3, 'max_depth': 15, 'learning_rate': 0.05}      0.9216482356147594   1465.747 0       \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 40%|████████████████████████████████                                                | 2/5 [48:24<1:12:30, 1450.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 900, 'min_child_weight': 3, 'max_depth': 10, 'learning_rate': 0.05}       0.9224307826020512   1439.056 1       \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 60%|████████████████████████████████████████████████                                | 3/5 [1:09:10<45:13, 1356.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 900, 'min_child_weight': 4, 'max_depth': 5, 'learning_rate': 0.1}         0.9221610009066519   1245.616 2       \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 80%|████████████████████████████████████████████████████████████████                | 4/5 [1:32:15<22:47, 1367.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 1100, 'min_child_weight': 1, 'max_depth': 2, 'learning_rate': 0.2}        0.9232741792710547   1384.956 3       \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 5/5 [2:00:03<00:00, 1440.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 1500, 'min_child_weight': 4, 'max_depth': 5, 'learning_rate': 0.05}       0.923078647175769    1668.232 4       \n",
      "      n_estimators    min_child_weight    max_depth    learning_rate        F1          Std     Time    No  Attack\n",
      "--  --------------  ------------------  -----------  ---------------  --------  -----------  -------  ----  --------\n",
      " 0            1100                   3           15             0.05  0.921648  1.11022e-16  1465.75     0  HPO\n",
      " 1             900                   3           10             0.05  0.922431  1.11022e-16  1439.06     1  HPO\n",
      " 2             900                   4            5             0.1   0.922161  0            1245.62     2  HPO\n",
      " 3            1100                   1            2             0.2   0.923274  0            1384.96     3  HPO\n",
      " 4            1500                   4            5             0.05  0.923079  0            1668.23     4  HPO\n",
      "      n_estimators   min_child_weight    max_depth     learning_rate     F1     Std     Time     No    Attack\n",
      "--  --------------  -----------  ---------------  ------------------  --------  -----  -------  ----  --------\n",
      " 0            1100            1                2                 0.2  0.923274      0  1384.96     3  HPO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import xgboost\n",
    "\n",
    "lines=[['n_estimators', 'min_child_weight', 'max_depth','learning_rate', \"F1\",\"Std\",\"Time\",\"No\",\"Attack\"]]\n",
    "\n",
    "for j in file_list:\n",
    "    print(j)\n",
    "    \n",
    "    \n",
    "    df=pd.read_csv(file_list[j][0],usecols=feature_list) \n",
    "    X_train = df.iloc[:,0:-1]\n",
    "    df['Label'] = df['Label'].astype('category')\n",
    "    y_train=df['Label'].cat.codes  \n",
    "\n",
    "\n",
    "    df=pd.read_csv(file_list[j][1],usecols=feature_list) \n",
    "    X_test = df.iloc[:,0:-1]\n",
    "    df['Label'] = df['Label'].astype('category')\n",
    "    y_test=df['Label'].cat.codes  \n",
    "    \n",
    "    X= np.concatenate([X_train, X_test])\n",
    "    test_fold = [-1 for _ in range(X_train.shape[0])] + [0 for _ in range(X_test.shape[0])]\n",
    "    y = np.concatenate([y_train, y_test])\n",
    "    ps = PredefinedSplit(test_fold)\n",
    "    \n",
    "\n",
    "\n",
    "    param_grid =   {\n",
    "    'n_estimators': [100, 500, 900, 1100, 1500],\n",
    "    'max_depth': [2, 3, 5, 10, 15],\n",
    "    'learning_rate': [0.05, 0.1, 0.15, 0.20],\n",
    "    'min_child_weight': [1, 2, 3, 4]\n",
    "    }\n",
    "\n",
    "    second=time()\n",
    "    f1=[]\n",
    "    clf=xgboost.XGBClassifier()\n",
    "    for ii in range(1):\n",
    "        clf.fit(X_train, y_train)\n",
    "        predict =clf.predict(X_test)\n",
    "        f1.append(sklearn.metrics.f1_score(y_test, predict,average= \"macro\") )\n",
    "    f1=sum(f1)/len(f1)   \n",
    "    #if f1>0.76:\n",
    "    print('%-35s %-20s %-8s %-8s' % (\"default\",f1,round(time()-second,3),ii))\n",
    "\n",
    "\n",
    "    \n",
    "    ######################################################################################################################\n",
    "    for i in tqdm(range(5)):\n",
    "        second=time()\n",
    "        a,b,clf=run_random_search(xgboost.XGBClassifier(),param_grid,X,y)\n",
    "        f1=[]\n",
    "        for ii in range(5):\n",
    "            clf.fit(X_train, y_train)\n",
    "            predict =clf.predict(X_test)\n",
    "            f1.append(sklearn.metrics.f1_score(y_test, predict,average= \"macro\") )\n",
    "        f1_result=sum(f1)/len(f1)   \n",
    "        f1=np.array(f1)\n",
    "        stndtd=f1.std()\n",
    "        temp=list(a.values())\n",
    "        print('%-90s %-20s %-8s %-8s' % (a,f1_result,round(time()-second,3),i))\n",
    "        temp=temp+[f1_result,stndtd,round(time()-second,3),i,j]\n",
    "        lines.append(temp)\n",
    "\n",
    "        #if f1>0.76:\n",
    "\n",
    "results = pd.DataFrame (lines[1:], columns = lines[0])\n",
    "results.to_csv(\"XGB_HPO.csv\",index=False)\n",
    "\n",
    "\n",
    "print (tabulate(results, headers=list(results.columns)))\n",
    "    \n",
    "final_parametres=[['n_estimators', 'min_child_weight', 'max_depth','learning_rate', \"F1\",\"Std\",\"Time\",\"No\",\"Attack\"]]\n",
    "for i in results[\"Attack\"].unique():\n",
    "    df=results[results[\"Attack\"]==i]\n",
    "    m=df[\"F1\"].max()\n",
    "    df=df[df[\"F1\"]==m]\n",
    "    m=df[\"max_depth\"].min()\n",
    "    df=df[df[\"max_depth\"]==m]  \n",
    "    final_parametres.append(list(df.values)[0])\n",
    "results = pd.DataFrame (final_parametres[1:], columns=  final_parametres[0])\n",
    "print (tabulate(results, headers=list(results.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomizedSearchCV  KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HPO\n",
      "default                             0.7656257145967937   196.639  24      \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 10/10 [37:46<00:00, 226.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    algorithm      leaf_size    n_neighbors  weights          F1    Std     Time    No  Attack\n",
      "--  -----------  -----------  -------------  ---------  --------  -----  -------  ----  --------\n",
      " 0  ball_tree             44              2  distance   0.789073      0  337.454     5  HPO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lines=[['algorithm', 'leaf_size', 'n_neighbors', 'weights', \"F1\",\"Std\",\"Time\",\"No\",\"Attack\"]]\n",
    "\n",
    "\n",
    "for j in file_list:\n",
    "    print(j)\n",
    "    \n",
    "    \n",
    "    df=pd.read_csv(file_list[j][0],usecols=feature_list) \n",
    "    X_train = df.iloc[:,0:-1]\n",
    "    df['Label'] = df['Label'].astype('category')\n",
    "    y_train=df['Label'].cat.codes  \n",
    "\n",
    "\n",
    "    df=pd.read_csv(file_list[j][1],usecols=feature_list) \n",
    "    X_test = df.iloc[:,0:-1]\n",
    "    df['Label'] = df['Label'].astype('category')\n",
    "    y_test=df['Label'].cat.codes  \n",
    "    \n",
    "    \n",
    "    \n",
    "    X= np.concatenate([X_train, X_test])\n",
    "    test_fold = [-1 for _ in range(X_train.shape[0])] + [0 for _ in range(X_test.shape[0])]\n",
    "    y = np.concatenate([y_train, y_test])\n",
    "    ps = PredefinedSplit(test_fold)\n",
    "    \n",
    "    \n",
    "\n",
    "    # use a full grid over all parameters\n",
    "    param_grid = {\"n_neighbors\" : sp_randint(1,64) ,  \n",
    "                 \"leaf_size\": sp_randint(1,50) , \n",
    "                  \"algorithm\" : [\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"],\n",
    "                  \"weights\" : [\"uniform\", \"distance\"]}\n",
    "    second=time()\n",
    "    f1=[]\n",
    "    clf=KNeighborsClassifier()\n",
    "    for ii in range(25):\n",
    "        clf.fit(X_train, y_train)\n",
    "        predict =clf.predict(X_test)\n",
    "        f1.append(sklearn.metrics.f1_score(y_test, predict,average= \"macro\") )\n",
    "    f1=sum(f1)/len(f1)   \n",
    "    #if f1>0.76:\n",
    "    print('%-35s %-20s %-8s %-8s' % (\"default\",f1,round(time()-second,3),ii))\n",
    "\n",
    "\n",
    "    \n",
    "    ######################################################################################################################\n",
    "    for i in tqdm(range(10)):\n",
    "        second=time()\n",
    "        a,b,clf=run_random_search(KNeighborsClassifier(),param_grid,X,y)\n",
    "        f1=[]\n",
    "        for ii in range(5):\n",
    "            clf.fit(X_train, y_train)\n",
    "            predict =clf.predict(X_test)\n",
    "            f1.append(sklearn.metrics.f1_score(y_test, predict,average= \"macro\") )\n",
    "        f1_result=sum(f1)/len(f1)   \n",
    "        f1=np.array(f1)\n",
    "        stndtd=f1.std()\n",
    "        temp=list(a.values())\n",
    "        #print('%-90s %-20s %-8s %-8s' % (a,f1_result,round(time()-second,3),i))\n",
    "        temp=temp+[f1_result,stndtd,round(time()-second,3),i,j]\n",
    "        lines.append(temp)\n",
    "\n",
    "        #if f1>0.76:\n",
    "results = pd.DataFrame (lines[1:], columns = lines[0])\n",
    "results.to_csv(\"knn_HPO.csv\",index=False)\n",
    "\n",
    "final_parametres=[['algorithm', 'leaf_size', 'n_neighbors', 'weights', \"F1\",\"Std\",\"Time\",\"No\",\"Attack\"]]\n",
    "\n",
    "for i in results[\"Attack\"].unique():\n",
    "    df=results[results[\"Attack\"]==i]\n",
    "    m=df[\"F1\"].max()\n",
    "    df=df[df[\"F1\"]==m]\n",
    "    m=df[\"n_neighbors\"].min()\n",
    "    df=df[df[\"n_neighbors\"]==m]  \n",
    "    final_parametres.append(list(df.values)[0])\n",
    "results = pd.DataFrame (final_parametres[1:], columns=  final_parametres[0])\n",
    "print (tabulate(results, headers=list(results.columns)))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HPO\n",
      "default                             0.1580460523385129   0.239    0       \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:40<00:00,  4.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      var_smoothing        F1    Std    Time    No  Attack\n",
      "--  ---------------  --------  -----  ------  ----  --------\n",
      " 0      8.11131e-09  0.149793      0   4.033     0  HPO\n",
      " 1      6.57933e-09  0.1485        0   4.064     1  HPO\n",
      " 2      1.51991e-09  0.146808      0   4.05      2  HPO\n",
      " 3      2.31013e-08  0.14379       0   4.065     3  HPO\n",
      " 4      1e-08        0.147798      0   4.051     4  HPO\n",
      " 5      8.11131e-09  0.149793      0   4.004     5  HPO\n",
      " 6      5.3367e-08   0.138853      0   4.07      6  HPO\n",
      " 7      3.51119e-08  0.141953      0   4.016     7  HPO\n",
      " 8      6.57933e-08  0.137392      0   4.06      8  HPO\n",
      " 9      5.3367e-07   0.130176      0   4.032     9  HPO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lines=[['var_smoothing', \"F1\",\"Std\",\"Time\",\"No\",\"Attack\"]]\n",
    "for j in file_list:\n",
    "    print(j)\n",
    "    \n",
    "    df=pd.read_csv(file_list[j][0],usecols=feature_list) \n",
    "    X_train = df.iloc[:,0:-1]\n",
    "    df['Label'] = df['Label'].astype('category')\n",
    "    y_train=df['Label'].cat.codes  \n",
    "\n",
    "\n",
    "    df=pd.read_csv(file_list[j][1],usecols=feature_list) \n",
    "    X_test = df.iloc[:,0:-1]\n",
    "    df['Label'] = df['Label'].astype('category')\n",
    "    y_test=df['Label'].cat.codes  \n",
    "    \n",
    "    X= np.concatenate([X_train, X_test])\n",
    "    test_fold = [-1 for _ in range(X_train.shape[0])] + [0 for _ in range(X_test.shape[0])]\n",
    "    y = np.concatenate([y_train, y_test])\n",
    "    ps = PredefinedSplit(test_fold)\n",
    "    \n",
    "    second=time()\n",
    "\n",
    "    param_grid = {\n",
    "        'var_smoothing': np.logspace(0,-9, num=100),\n",
    "    }\n",
    "\n",
    " \n",
    "    second=time()\n",
    "    f1=[]\n",
    "    clf=GaussianNB()\n",
    "    for ii in range(1):\n",
    "        clf.fit(X_train, y_train)\n",
    "        predict =clf.predict(X_test)\n",
    "        f1.append(sklearn.metrics.f1_score(y_test, predict,average= \"macro\") )\n",
    "    f1=sum(f1)/len(f1)   \n",
    "    #if f1>0.76:\n",
    "    print('%-35s %-20s %-8s %-8s' % (\"default\",f1,round(time()-second,3),ii))\n",
    "    ######################################################################################################################\n",
    "    for i in tqdm(range(10)):\n",
    "        second=time()\n",
    "        a,b,clf=run_random_search(GaussianNB(),param_grid,X,y)\n",
    "        f1=[]\n",
    "        for ii in range(5):\n",
    "            clf.fit(X_train, y_train)\n",
    "            predict =clf.predict(X_test)\n",
    "            f1.append(sklearn.metrics.f1_score(y_test, predict,average= \"macro\") )\n",
    "        f1_result=sum(f1)/len(f1)   \n",
    "        f1=np.array(f1)\n",
    "        stndtd=f1.std()\n",
    "        temp=list(a.values())\n",
    "        #print('%-90s %-20s %-8s %-8s' % (a,f1_result,round(time()-second,3),i))\n",
    "        temp=temp+[f1_result,stndtd,round(time()-second,3),i,j]\n",
    "        lines.append(temp)\n",
    "\n",
    "\n",
    "results = pd.DataFrame (lines[1:], columns = lines[0])\n",
    "results.to_csv(\"NB_HPO.csv\",index=False)\n",
    "print (tabulate(results, headers=list(results.columns)))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      var_smoothing        F1    Std    Time    No  Attack\n",
      "--  ---------------  --------  -----  ------  ----  --------\n",
      " 0      8.11131e-09  0.149793      0   4.033     0  HPO\n"
     ]
    }
   ],
   "source": [
    "\n",
    "final_parametres=[[\"var_smoothing\",\"F1\",\"Std\",\"Time\",\"No\",\"Attack\"]]\n",
    "for i in results[\"Attack\"].unique():\n",
    "    df=results[results[\"Attack\"]==i]\n",
    "    m=df[\"F1\"].max()\n",
    "    df=df[df[\"F1\"]==m]\n",
    "    final_parametres.append(list(df.values)[0])\n",
    "results = pd.DataFrame (final_parametres[1:], columns=  final_parametres[0])\n",
    "print (tabulate(results, headers=list(results.columns)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HPO\n",
      "default                             0.11182048830398614  3.961    0       \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████▌                                     | 5/10 [12:21:03<12:21:03, 8892.60s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m)):\n\u001b[0;32m     45\u001b[0m     second\u001b[38;5;241m=\u001b[39mtime()\n\u001b[1;32m---> 46\u001b[0m     a,b,clf\u001b[38;5;241m=\u001b[39m\u001b[43mrun_random_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mLogisticRegression\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mparam_grid\u001b[49m\u001b[43m,\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m     f1\u001b[38;5;241m=\u001b[39m[]\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ii \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m):\n",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36mrun_random_search\u001b[1;34m(model, params, x_train, y_train)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_random_search\u001b[39m(model, params, x_train, y_train):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m#grid = GridSearchCV(model, params, cv = ps, n_jobs = -1, scoring = score, verbose = 0, refit = False)\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     grid \u001b[38;5;241m=\u001b[39mRandomizedSearchCV(model, param_grid, cv\u001b[38;5;241m=\u001b[39mps,scoring \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1_macro\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m     \u001b[43mgrid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (grid\u001b[38;5;241m.\u001b[39mbest_params_, \u001b[38;5;28mround\u001b[39m(grid\u001b[38;5;241m.\u001b[39mbest_score_,\u001b[38;5;241m8\u001b[39m),grid\u001b[38;5;241m.\u001b[39mbest_estimator_)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:891\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    885\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    886\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    887\u001b[0m     )\n\u001b[0;32m    889\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 891\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    893\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    894\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    895\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1766\u001b[0m, in \u001b[0;36mRandomizedSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1764\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1765\u001b[0m     \u001b[38;5;124;03m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1766\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1767\u001b[0m \u001b[43m        \u001b[49m\u001b[43mParameterSampler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1768\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_distributions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\n\u001b[0;32m   1769\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1770\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:838\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    830\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    831\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    832\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    833\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    834\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    835\u001b[0m         )\n\u001b[0;32m    836\u001b[0m     )\n\u001b[1;32m--> 838\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    839\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    840\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    841\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    842\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    843\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    844\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    845\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    846\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    848\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    849\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    850\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    851\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    852\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    853\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    855\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    856\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    857\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    858\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    859\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    860\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:1046\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1043\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1044\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1046\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1047\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre_dispatch \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1050\u001b[0m     \u001b[38;5;66;03m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1051\u001b[0m     \u001b[38;5;66;03m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1052\u001b[0m     \u001b[38;5;66;03m# consumption.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:861\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    860\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 861\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:779\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    777\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    778\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[1;32m--> 779\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    780\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    781\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    782\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    783\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    784\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mImmediateResult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\_parallel_backends.py:572\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    569\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[0;32m    570\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    571\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 572\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:262\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 262\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    263\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:262\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 262\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    263\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\fixes.py:216\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig):\n\u001b[1;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:680\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    678\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    679\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 680\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    682\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    683\u001b[0m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[0;32m    684\u001b[0m     fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1528\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m effective_n_jobs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1523\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1524\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_jobs\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m > 1 does not have any effect when\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msolver\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is set to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mliblinear\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Got \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_jobs\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m = \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(effective_n_jobs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs))\n\u001b[0;32m   1527\u001b[0m     )\n\u001b[1;32m-> 1528\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercept_, n_iter_ \u001b[38;5;241m=\u001b[39m \u001b[43m_fit_liblinear\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1530\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1531\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1532\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_intercept\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1533\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintercept_scaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1534\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1535\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpenalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1536\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdual\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1537\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1538\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1539\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1540\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1541\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1542\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([n_iter_])\n\u001b[0;32m   1544\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1186\u001b[0m, in \u001b[0;36m_fit_liblinear\u001b[1;34m(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state, multi_class, loss, epsilon, sample_weight)\u001b[0m\n\u001b[0;32m   1183\u001b[0m sample_weight \u001b[38;5;241m=\u001b[39m _check_sample_weight(sample_weight, X, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[0;32m   1185\u001b[0m solver_type \u001b[38;5;241m=\u001b[39m _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n\u001b[1;32m-> 1186\u001b[0m raw_coef_, n_iter_ \u001b[38;5;241m=\u001b[39m \u001b[43mliblinear\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_wrap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1187\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1188\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_ind\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1189\u001b[0m \u001b[43m    \u001b[49m\u001b[43msp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misspmatrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1190\u001b[0m \u001b[43m    \u001b[49m\u001b[43msolver_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1191\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1194\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1195\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1196\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrnd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mi\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1197\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1198\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1199\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1200\u001b[0m \u001b[38;5;66;03m# Regarding rnd.randint(..) in the above signature:\u001b[39;00m\n\u001b[0;32m   1201\u001b[0m \u001b[38;5;66;03m# seed for srand in range [0..INT_MAX); due to limitations in Numpy\u001b[39;00m\n\u001b[0;32m   1202\u001b[0m \u001b[38;5;66;03m# on 32-bit platforms, we can't get to the UINT_MAX limit that\u001b[39;00m\n\u001b[0;32m   1203\u001b[0m \u001b[38;5;66;03m# srand supports\u001b[39;00m\n\u001b[0;32m   1204\u001b[0m n_iter_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(n_iter_)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from scipy.stats import loguniform\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lines=[[\"C\",'penalty','Solver',  \"F1\",\"Std\",\"Time\",\"No\",\"Attack\"]]\n",
    "for j in file_list:\n",
    "    print(j)\n",
    "    \n",
    "    df=pd.read_csv(file_list[j][0],usecols=feature_list) \n",
    "    X_train = df.iloc[:,0:-1]\n",
    "    df['Label'] = df['Label'].astype('category')\n",
    "    y_train=df['Label'].cat.codes  \n",
    "\n",
    "\n",
    "    df=pd.read_csv(file_list[j][1],usecols=feature_list) \n",
    "    X_test = df.iloc[:,0:-1]\n",
    "    df['Label'] = df['Label'].astype('category')\n",
    "    y_test=df['Label'].cat.codes  \n",
    "    \n",
    "    X= np.concatenate([X_train, X_test])\n",
    "    test_fold = [-1 for _ in range(X_train.shape[0])] + [0 for _ in range(X_test.shape[0])]\n",
    "    y = np.concatenate([y_train, y_test])\n",
    "    ps = PredefinedSplit(test_fold)\n",
    "    \n",
    "    second=time()\n",
    "\n",
    "    param_grid = {\n",
    "     'solver': ['newton-cg', 'lbfgs', 'liblinear'],\n",
    "    'penalty' : ['none', 'l1', 'l2', 'elasticnet'],\n",
    "    'C' : loguniform(1e-5, 100)\n",
    "        }\n",
    "    \n",
    "\n",
    " \n",
    "    second=time()\n",
    "    f1=[]\n",
    "    clf=LogisticRegression()\n",
    "    for ii in range(1):\n",
    "        clf.fit(X_train, y_train)\n",
    "        predict =clf.predict(X_test)\n",
    "        f1.append(sklearn.metrics.f1_score(y_test, predict,average= \"macro\") )\n",
    "    f1=sum(f1)/len(f1)   \n",
    "    #if f1>0.76:\n",
    "    print('%-35s %-20s %-8s %-8s' % (\"default\",f1,round(time()-second,3),ii))\n",
    "    ######################################################################################################################\n",
    "    for i in tqdm(range(10)):\n",
    "        second=time()\n",
    "        a,b,clf=run_random_search(LogisticRegression(),param_grid,X,y)\n",
    "        f1=[]\n",
    "        for ii in range(5):\n",
    "            clf.fit(X_train, y_train)\n",
    "            predict =clf.predict(X_test)\n",
    "            f1.append(sklearn.metrics.f1_score(y_test, predict,average= \"macro\") )\n",
    "        f1_result=sum(f1)/len(f1)   \n",
    "        f1=np.array(f1)\n",
    "        stndtd=f1.std()\n",
    "        temp=list(a.values())\n",
    "        #print('%-90s %-20s %-8s %-8s' % (a,f1_result,round(time()-second,3),i))\n",
    "        temp=temp+[f1_result,stndtd,round(time()-second,3),i,j]\n",
    "        lines.append(temp)\n",
    "\n",
    "\n",
    "results = pd.DataFrame (lines[1:], columns = lines[0])\n",
    "results.to_csv(\"LR_HPO.csv\",index=False)\n",
    "print (tabulate(results, headers=list(results.columns)))\n",
    "    \n",
    "final_parametres=[[\"C\",'penalty','Solver',  \"F1\",\"Std\",\"Time\",\"No\",\"Attack\"]]\n",
    "for i in results[\"Attack\"].unique():\n",
    "    df=results[results[\"Attack\"]==i]\n",
    "    m=df[\"F1\"].max()\n",
    "    df=df[df[\"F1\"]==m]\n",
    "    final_parametres.append(list(df.values)[0])\n",
    "results = pd.DataFrame (final_parametres[1:], columns=  final_parametres[0])\n",
    "print (tabulate(results, headers=list(results.columns)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             C  penalty    Solver           F1          Std      Time    No  Attack\n",
      "--  ----------  ---------  ---------  --------  -----------  --------  ----  --------\n",
      " 0  0.117689    l1         liblinear  0.689609  0.00218566    9837.88     0  HPO\n",
      " 1  2.1957e-05  none       newton-cg  0.277286  0             1084.27     1  HPO\n",
      " 2  0.071026    l1         liblinear  0.691568  0.00296031    9556.38     2  HPO\n",
      " 3  0.074702    l1         liblinear  0.690907  0.00137433    9772.06     3  HPO\n",
      " 4  0.00314367  l1         liblinear  0.651044  0.000708044  12860.2      4  HPO\n"
     ]
    }
   ],
   "source": [
    "results = pd.DataFrame (lines[1:], columns = lines[0])\n",
    "results.to_csv(\"LR_HPO.csv\",index=False)\n",
    "print (tabulate(results, headers=list(results.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           C  penalty    Solver           F1         Std     Time    No  Attack\n",
      "--  --------  ---------  ---------  --------  ----------  -------  ----  --------\n",
      " 0  0.071026  l1         liblinear  0.691568  0.00296031  9556.38     2  HPO\n"
     ]
    }
   ],
   "source": [
    "final_parametres=[[\"C\",'penalty','Solver',  \"F1\",\"Std\",\"Time\",\"No\",\"Attack\"]]\n",
    "for i in results[\"Attack\"].unique():\n",
    "    df=results[results[\"Attack\"]==i]\n",
    "    m=df[\"F1\"].max()\n",
    "    df=df[df[\"F1\"]==m]\n",
    "    final_parametres.append(list(df.values)[0])\n",
    "results = pd.DataFrame (final_parametres[1:], columns=  final_parametres[0])\n",
    "print (tabulate(results, headers=list(results.columns)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomizedSearchCV  SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HPO\n",
      "default                             0.10090979652726553  115.611  0       \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 1/1 [34:33<00:00, 2073.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.001, 'C': 10}                                                                  0.16788597766513827  2073.234 0       \n",
      "      gamma    C        F1    Std     Time    No  Attack\n",
      "--  -------  ---  --------  -----  -------  ----  --------\n",
      " 0    0.001   10  0.167886      0  2073.23     0  HPO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lines=[[ 'gamma','C', \"F1\",\"Std\",\"Time\",\"No\",\"Attack\"]]\n",
    "\n",
    "\n",
    "for j in file_list:\n",
    "    print(j)\n",
    "    \n",
    "    \n",
    "    df=pd.read_csv(file_list[j][0],usecols=feature_list) \n",
    "    X_train = df.iloc[:,0:-1]\n",
    "    df['Label'] = df['Label'].astype('category')\n",
    "    y_train=df['Label'].cat.codes  \n",
    "\n",
    "\n",
    "    df=pd.read_csv(file_list[j][1],usecols=feature_list) \n",
    "    X_test = df.iloc[:,0:-1]\n",
    "    df['Label'] = df['Label'].astype('category')\n",
    "    y_test=df['Label'].cat.codes  \n",
    "    \n",
    "    X= np.concatenate([X_train, X_test])\n",
    "    test_fold = [-1 for _ in range(X_train.shape[0])] + [0 for _ in range(X_test.shape[0])]\n",
    "    y = np.concatenate([y_train, y_test])\n",
    "    ps = PredefinedSplit(test_fold)\n",
    "    \n",
    "\n",
    "\n",
    "    param_grid =  {'C': [0.001, 0.01, 0.1, 1, 10], 'gamma' : [0.001, 0.01, 0.1, 1]}  \n",
    "\n",
    "    second=time()\n",
    "    f1=[]\n",
    "    clf=svm.SVC()\n",
    "    for ii in range(1):\n",
    "        clf.fit(X_train, y_train)\n",
    "        predict =clf.predict(X_test)\n",
    "        f1.append(sklearn.metrics.f1_score(y_test, predict,average= \"macro\") )\n",
    "    f1=sum(f1)/len(f1)   \n",
    "    #if f1>0.76:\n",
    "    print('%-35s %-20s %-8s %-8s' % (\"default\",f1,round(time()-second,3),ii))\n",
    "\n",
    "\n",
    "    \n",
    "    ######################################################################################################################\n",
    "    for i in tqdm(range(1)):\n",
    "        second=time()\n",
    "        a,b,clf=run_random_search(svm.SVC(),param_grid,X,y)\n",
    "        f1=[]\n",
    "        for ii in range(1):\n",
    "            clf.fit(X_train, y_train)\n",
    "            predict =clf.predict(X_test)\n",
    "            f1.append(sklearn.metrics.f1_score(y_test, predict,average= \"macro\") )\n",
    "        f1_result=sum(f1)/len(f1)   \n",
    "        f1=np.array(f1)\n",
    "        stndtd=f1.std()\n",
    "        temp=list(a.values())\n",
    "        print('%-90s %-20s %-8s %-8s' % (a,f1_result,round(time()-second,3),i))\n",
    "        temp=temp+[f1_result,stndtd,round(time()-second,3),i,j]\n",
    "        lines.append(temp)\n",
    "\n",
    "        #if f1>0.76:\n",
    "\n",
    "results = pd.DataFrame (lines[1:], columns = lines[0])\n",
    "results.to_csv(\"svm_HPO.csv\",index=False)\n",
    "\n",
    "\n",
    "print (tabulate(results, headers=list(results.columns)))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "!shutdown /s /t 360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
